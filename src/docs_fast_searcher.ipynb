{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6ee785",
   "metadata": {},
   "source": [
    "# RAG-based Assistent to fastly query AWS Documentation\n",
    "\n",
    "In this notebook we will make the setup of the pipeline and answer the four questions proposed in the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d816356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewerneck/.pyenv/versions/3.13.3/envs/lokaenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from vector_store import VectorStore\n",
    "from dense_embedder import DenseEmbedder\n",
    "from sparse_embedder import BM25Retriever\n",
    "from generator import LLMGenerator\n",
    "from config import MAX_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764e8df",
   "metadata": {},
   "source": [
    "**IMPORTANT:** run the build_index.py file for chunking and indexing the data if you did not clone the data files from the repo or if you want to update the indexing.\n",
    "\n",
    "you can do this by : python3 src/build_index.py in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f526ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux function to format the prompt\n",
    "def format_prompt(query: str, contexts: list[dict]) -> str:\n",
    "    \"\"\"Constructs the prompt with retrieved contexts and user query.\"\"\"\n",
    "    context_str = \"\\n\\n\".join(\n",
    "        f\"Chunk {i+1}:\\n{chunk['text'].strip()}\"\n",
    "        for i, chunk in enumerate(contexts)\n",
    "    )\n",
    "\n",
    "    return f\"\"\"You are a helpful assistant answering questions about AWS SageMaker documentation. Base your answers on the provided context.\n",
    "If the context does not contain enough information, you can answer with \"I don't know\".\n",
    "Lastly, if you are sure about the answer, you can answer even if the information is not in the context.\n",
    "There is always just one question, so do not answer multiple questions at once.\n",
    "Give concise and accurate answers.\n",
    "\n",
    "Context:\n",
    "{context_str}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201cf861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "dense_embedder = DenseEmbedder()\n",
    "sparse_embedder = BM25Retriever()\n",
    "generator = LLMGenerator()\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d864b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Get user query ---\n",
    "### Test any query you want here\n",
    "query = \"What is SageMaker?\"\n",
    "\n",
    "# --- Step 2: Retrieve from dense store ---\n",
    "query_vec = dense_embedder.encode([query])\n",
    "store = VectorStore(dim=query_vec.shape[1])\n",
    "store.load(\"../data\")\n",
    "dense_results = store.search(query_vec, top_k=5)\n",
    "dense_chunks = [meta for _, _, meta in dense_results]\n",
    "\n",
    "# --- Step 3: Retrieve from sparse BM25 ---\n",
    "sparse_embedder.load(\"../data\")\n",
    "sparse_results = sparse_embedder.search(query, top_k=5)\n",
    "sparse_chunks = [meta for _, meta in sparse_results]\n",
    "\n",
    "# --- Step 4: Merge and deduplicate ---\n",
    "all_candidates = {doc[\"id\"]: doc for doc in dense_chunks + sparse_chunks}\n",
    "candidate_chunks = list(all_candidates.values())\n",
    "\n",
    "# --- Step 5: Rerank ---\n",
    "rerank_inputs = [(query, chunk[\"text\"]) for chunk in candidate_chunks]\n",
    "scores = reranker.predict(rerank_inputs)\n",
    "\n",
    "for chunk, score in zip(candidate_chunks, scores):\n",
    "    chunk[\"score\"] = float(score)\n",
    "\n",
    "top_reranked = sorted(candidate_chunks, key=lambda x: x[\"score\"], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a4908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1268\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "Answer:\n",
      "\n",
      "Amazon SageMaker is a fully managed service that enables developers and data scientists to build, train, and deploy machine learning models. It provides integrated Jupyter authoring notebook instances for easy access to data sources and eliminates the need to manage servers.\n",
      "\n",
      "\n",
      "Source for the answer: examples-sagemaker.md\n",
      "\n",
      "\n",
      "Other possible relevant sources for further reading: integrating-sagemaker.md, sagemaker-projects-whatis.md, kubernetes-sagemaker-jobs.md, sagemaker-projects.md\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Format and generate response ---\n",
    "prompt = format_prompt(query, top_reranked)\n",
    "print(\"Token count:\", generator.count_tokens(prompt))\n",
    "\n",
    "print(\"\\nGenerating answer...\\n\")\n",
    "answer = generator.llmgenerate(prompt=prompt, max_tokens=MAX_TOKENS)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(f'Source for the answer: {top_reranked[0][\"source\"]}')\n",
    "print('\\n')\n",
    "print(f\"Other possible relevant sources for further reading: {', '.join(chunk['source'] for chunk in top_reranked[1:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc054c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9841cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Get user query ---\n",
    "### Test any query you want here\n",
    "query = \"What are all AWS regions where SageMaker is available?\"\n",
    "\n",
    "# --- Step 2: Retrieve from dense store ---\n",
    "query_vec = dense_embedder.encode([query])\n",
    "store = VectorStore(dim=query_vec.shape[1])\n",
    "store.load(\"../data\")\n",
    "dense_results = store.search(query_vec, top_k=5)\n",
    "dense_chunks = [meta for _, _, meta in dense_results]\n",
    "\n",
    "# --- Step 3: Retrieve from sparse BM25 ---\n",
    "sparse_embedder.load(\"../data\")\n",
    "sparse_results = sparse_embedder.search(query, top_k=5)\n",
    "sparse_chunks = [meta for _, meta in sparse_results]\n",
    "\n",
    "# --- Step 4: Merge and deduplicate ---\n",
    "all_candidates = {doc[\"id\"]: doc for doc in dense_chunks + sparse_chunks}\n",
    "candidate_chunks = list(all_candidates.values())\n",
    "\n",
    "# --- Step 5: Rerank ---\n",
    "rerank_inputs = [(query, chunk[\"text\"]) for chunk in candidate_chunks]\n",
    "scores = reranker.predict(rerank_inputs)\n",
    "\n",
    "for chunk, score in zip(candidate_chunks, scores):\n",
    "    chunk[\"score\"] = float(score)\n",
    "\n",
    "top_reranked = sorted(candidate_chunks, key=lambda x: x[\"score\"], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248f1466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1791\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "Answer:\n",
      "\n",
      "SageMaker is available in all supported AWS regions except Asia Pacific (Jakarta), Africa (Cape Town), Middle East (UAE), Asia Pacific (Hyderabad), Asia Pacific (Osaka), Asia Pacific (Melbourne), Europe (Milan), AWS GovCloud (US-East), Europe (Spain), China (Beijing), China (Ningxia), and Europe (Zurich) Region.\n",
      "\n",
      "\n",
      "Source for the answer: sagemaker-notebook-no-direct-internet-access.md\n",
      "\n",
      "\n",
      "Other possible relevant sources for further reading: sagemaker-notebook-instance-inside-vpc.md, sagemaker-compliance.md, aws-properties-sagemaker-model-containerdefinition.md, sagemaker-projects-whatis.md\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Format and generate response ---\n",
    "prompt = format_prompt(query, top_reranked)\n",
    "print(\"Token count:\", generator.count_tokens(prompt))\n",
    "\n",
    "print(\"\\nGenerating answer...\\n\")\n",
    "answer = generator.llmgenerate(prompt=prompt, max_tokens=MAX_TOKENS)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(f'Source for the answer: {top_reranked[0][\"source\"]}')\n",
    "print('\\n')\n",
    "print(f\"Other possible relevant sources for further reading: {', '.join(chunk['source'] for chunk in top_reranked[1:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a081a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6816ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Get user query ---\n",
    "### Test any query you want here\n",
    "query = \"How to check if an endpoint is KMS encrypted?\"\n",
    "\n",
    "# --- Step 2: Retrieve from dense store ---\n",
    "query_vec = dense_embedder.encode([query])\n",
    "store = VectorStore(dim=query_vec.shape[1])\n",
    "store.load(\"../data\")\n",
    "dense_results = store.search(query_vec, top_k=5)\n",
    "dense_chunks = [meta for _, _, meta in dense_results]\n",
    "\n",
    "# --- Step 3: Retrieve from sparse BM25 ---\n",
    "sparse_embedder.load(\"../data\")\n",
    "sparse_results = sparse_embedder.search(query, top_k=5)\n",
    "sparse_chunks = [meta for _, meta in sparse_results]\n",
    "\n",
    "# --- Step 4: Merge and deduplicate ---\n",
    "all_candidates = {doc[\"id\"]: doc for doc in dense_chunks + sparse_chunks}\n",
    "candidate_chunks = list(all_candidates.values())\n",
    "\n",
    "# --- Step 5: Rerank ---\n",
    "rerank_inputs = [(query, chunk[\"text\"]) for chunk in candidate_chunks]\n",
    "scores = reranker.predict(rerank_inputs)\n",
    "\n",
    "for chunk, score in zip(candidate_chunks, scores):\n",
    "    chunk[\"score\"] = float(score)\n",
    "\n",
    "top_reranked = sorted(candidate_chunks, key=lambda x: x[\"score\"], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22ef56ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 2011\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "Answer:\n",
      "\n",
      "You can check the compliance of an Amazon SageMaker endpoint configuration regarding KMS encryption using AWS Config rules such as 'sagemaker-endpoint-configuration-kms-key-configured'. If the rule returns NON_COMPLIANT, then the KMS key is not configured for the endpoint configuration.\n",
      "\n",
      "\n",
      "Source for the answer: sagemaker-roles.md\n",
      "\n",
      "\n",
      "Other possible relevant sources for further reading: sagemaker-endpoint-configuration-kms-key-configured.md, aws-properties-sagemaker-featuregroup-onlinestoreconfig.md, aws-properties-sagemaker-modelpackage-transformresources.md, kubernetes-sagemaker-components-tutorials.md\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Format and generate response ---\n",
    "prompt = format_prompt(query, top_reranked)\n",
    "print(\"Token count:\", generator.count_tokens(prompt))\n",
    "\n",
    "print(\"\\nGenerating answer...\\n\")\n",
    "answer = generator.llmgenerate(prompt=prompt, max_tokens=MAX_TOKENS)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(f'Source for the answer: {top_reranked[0][\"source\"]}')\n",
    "print('\\n')\n",
    "print(f\"Other possible relevant sources for further reading: {', '.join(chunk['source'] for chunk in top_reranked[1:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29d75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9f4b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Get user query ---\n",
    "### Test any query you want here\n",
    "query = \"What are SageMaker Geospatial capabilities?\"\n",
    "\n",
    "# --- Step 2: Retrieve from dense store ---\n",
    "query_vec = dense_embedder.encode([query])\n",
    "store = VectorStore(dim=query_vec.shape[1])\n",
    "store.load(\"../data\")\n",
    "dense_results = store.search(query_vec, top_k=5)\n",
    "dense_chunks = [meta for _, _, meta in dense_results]\n",
    "\n",
    "# --- Step 3: Retrieve from sparse BM25 ---\n",
    "sparse_embedder.load(\"../data\")\n",
    "sparse_results = sparse_embedder.search(query, top_k=5)\n",
    "sparse_chunks = [meta for _, meta in sparse_results]\n",
    "\n",
    "# --- Step 4: Merge and deduplicate ---\n",
    "all_candidates = {doc[\"id\"]: doc for doc in dense_chunks + sparse_chunks}\n",
    "candidate_chunks = list(all_candidates.values())\n",
    "\n",
    "# --- Step 5: Rerank ---\n",
    "rerank_inputs = [(query, chunk[\"text\"]) for chunk in candidate_chunks]\n",
    "scores = reranker.predict(rerank_inputs)\n",
    "\n",
    "for chunk, score in zip(candidate_chunks, scores):\n",
    "    chunk[\"score\"] = float(score)\n",
    "\n",
    "top_reranked = sorted(candidate_chunks, key=lambda x: x[\"score\"], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "983f8a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1355\n",
      "\n",
      "Generating answer...\n",
      "\n",
      "Answer:\n",
      "\n",
      "SageMaker Geospatial capabilities are features of Amazon SageMaker that perform geospatial operations on your behalf using the AWS hardware managed by SageMaker. They can only perform operations that the user permits and require an execution role with the appropriate permissions to access AWS resources.\n",
      "\n",
      "\n",
      "Source for the answer: sagemaker-geospatial-roles.md\n",
      "\n",
      "\n",
      "Other possible relevant sources for further reading: sagemaker-geospatial-roles.md, integrating-sagemaker.md, examples-sagemaker.md, sagemaker-projects-whatis.md\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Format and generate response ---\n",
    "prompt = format_prompt(query, top_reranked)\n",
    "print(\"Token count:\", generator.count_tokens(prompt))\n",
    "\n",
    "print(\"\\nGenerating answer...\\n\")\n",
    "answer = generator.llmgenerate(prompt=prompt, max_tokens=MAX_TOKENS)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(f'Source for the answer: {top_reranked[0][\"source\"]}')\n",
    "print('\\n')\n",
    "print(f\"Other possible relevant sources for further reading: {', '.join(chunk['source'] for chunk in top_reranked[1:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d26c71",
   "metadata": {},
   "source": [
    "You can see that the answers are quite good and are also well related to the documentation. For sure, with better models the whole pipeline would be better, from chunking to generating the answer. But anyway I would say that this POC accomplished its objectives by generating good enough answers in a reasonable time even running in my local machine (macbook air M4 16gb RAM 256 SSD). Being more specific it took me for each query something between 10 and 40 seconds to have an answer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lokaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
