# Documentation Q&A Assistant

This project is a lightweight, modular proof-of-concept (PoC) system designed to assist developers in navigating large technical documentation efficiently. It targets the common pain point of developers spending too much time searching for information or relying on colleagues for answers that are often buried in internal documents.

The current implementation uses AWS documentation to simulate this scenario and demonstrates a pipeline that integrates preprocessing, hybrid document retrieval (sparse + dense), and local large language model (LLM) generation.

---

## Project Motivation

Engineering teams often face productivity bottlenecks when navigating vast and complex documentation. This project aims to:
	â€¢	Provide instant, accurate answers to user questions based on existing documentation.
	â€¢	Reduce interruptions to experienced engineers.
	â€¢	Ensure responses remain consistent and up-to-date with the latest documents.

While the primary use case is search and Q&A over AWS documentation, the architecture is built with flexibility and scalability in mind, enabling easy extension to internal, private datasets.

---

## Architecture Overview

```text
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   User Query     â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Hybrid Retrieval (Dense + â”‚
        â”‚ Sparse + Deduplication)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  CrossEncoder   â”‚  â†’ Rerank by relevance
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
               Top-k Relevant Chunks
                      â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚   Prompt + LLM  â”‚  â†’ Mistral-7B (local)
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              Final Answer + Sources
```

The system follows a Retrieval-Augmented Generation (RAG) pattern, which combines a document retriever and a text generator:    
1.	Preprocessing & Chunking (preprocessing.py)
Markdown files are parsed into clean text, split into manageable chunks, and saved with metadata for later retrieval.
2.	Indexing Phase (build_index.py)
	â€¢	Dense embeddings are generated using sentence-transformers.
	â€¢	Sparse BM25 vectors are computed using rank_bm25.
	â€¢	Metadata, chunks, and indexes are stored to disk.
3.	Retrieval Layer
	â€¢	sparse_embedder.py: Performs keyword-based retrieval with BM25.
	â€¢	dense_embedder.py: Retrieves semantically relevant chunks.
	â€¢	vector_store.py: Uses FAISS to index and encapsulate logic for vector semantic search with cosine-similarity.
4.	Reranking (Late Fusion) (generator.py)
Retrieved results are reranked using a cross-encoder model (cross-encoder/ms-marco-MiniLM-L6-v2) to select the most relevant chunks.
5.	Prompt Construction + LLM Response
Top-k reranked chunks are used to form the prompt.
Generation is handled locally via llama-cpp-python, running quantized Mistral-7B-Instruct in GGUF format from the models/ directory.

---

### Project Structure
```
.
â”œâ”€â”€ data/                  # Stored embeddings, chunks, and indexes
â”œâ”€â”€ models/                # Local model files (GGUF)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ init.py                  # Source code
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ build_index.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ dense_embedder.py
â”‚   â”œâ”€â”€ sparse_embedder.py
â”‚   â”œâ”€â”€ evaluate_retrieval.py
â”‚   â”œâ”€â”€ generator.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ docs_fast_searcher.ipynb  # notebook for seeing answers for suggested questions and to play with the solution
â”œâ”€â”€ llm_install.py         # Local model loader using llama-cpp
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ README.md
```

â¸»

ğŸš€ How to Run the Project

âš™ï¸ Setup

Make sure you have Python 3.10+ installed. Then run:

bash setup.sh

This script will:
	â€¢	Create a virtual environment
	â€¢	Install dependencies
	â€¢	Download necessary NLTK packages

Alternatively, install everything manually:

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt')"

ğŸ§± Building the Index

python src/build_index.py

This will process the raw documents, compute embeddings, and store indexes in the data/ folder.

ğŸ§ª Running the QA Pipeline

You can test the full retrieval + generation flow using:

python src/main.py

Youâ€™ll be prompted to enter a question.

â¸»

ğŸ§° Tooling Choices Justified
	â€¢	sentence-transformers + FAISS: Efficient dense vector search; captures semantic similarity.
	â€¢	rank_bm25: Lightweight sparse retriever to complement dense search.
	â€¢	Hybrid Retrieval + Cross-Encoder Reranker: Increases precision of retrieved results.
	â€¢	llama-cpp-python with Mistral-7B-Instruct GGUF: Allows fully local LLM inference, critical for privacy and geographical constraints.

This design ensures flexibility across use cases, including ones with sensitive or private data.

â¸»

ğŸ§  Design Decisions
	â€¢	Chunking Strategy: Documents are split by paragraphs and further chunked into 300-400 token windows. This provides contextual coverage without exceeding LLM context limits.
	â€¢	Late Fusion Reranking: Dense and sparse results are merged and then re-ranked using a cross-encoder for optimal relevance.
	â€¢	Local Model Inference: Chosen to avoid data leakage and support air-gapped environments.

â¸»

ğŸ§ª Evaluation

Evaluation scripts (evaluate_retrieval.py) allow assessment of retrieval quality using a small QA set. Metrics like hit rate, top-k coverage, and reranking gain are considered. This can be extended with ground-truth annotations or relevance feedback in a production setting.

â¸»

âœ… Whatâ€™s Production-Ready?
	â€¢	Modular pipeline: easy to adapt/extend.
	â€¢	Local model loading and offline inference.
	â€¢	Efficient document indexing and hybrid search.

ğŸš§ Whatâ€™s Missing
	â€¢	No web interface or API (could be added via FastAPI)
	â€¢	No continuous ingestion pipeline for updated documents.
	â€¢	No fine-tuning or domain-specific retriever optimization.

â¸»

ğŸ”„ Updating the Knowledge Base

The system can be re-indexed periodically using the same build_index.py script. Future iterations could support real-time indexing and invalidation of outdated chunks.

â¸»

ğŸ§ª Tests

Basic tests are included in the tests/ folder to validate key components of the system such as the generator pipeline.

Run tests using:

pytest tests/




â¸»

ğŸ¤ Contributions

Pull requests and suggestions welcome. This project is meant to grow with evolving needs in document Q&A and retrieval systems.

â¸»

ğŸ“œ License

MIT License.